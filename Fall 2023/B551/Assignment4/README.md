#  fraramir-nhaldert-pcoen-a3
#### How you divided the work among team members
  - Everyone worked together on both parts.
  - Some members focused more on part 1 than part 2 (and viceversa), but we all contributed to both parts regardless.
#### The contribution of each team member
 - Everyone:
   - Everyone contributed to the idea/design of each of these methods and checking work implemented by everyone.
 - Frangil Ramirez Koteich (fraramir)
   - Data pre-processing for Part 2 and implementation of simplified model for Part 2.
   - Part 2 report for the above tasks (pre-processing and simplified model).
 - Nilambar Halder Tonmoy (nhalder)
   - Calculating transition probabilities and initial transition probabilities for part2 hmm method
   - Implementing prediction method for test letters using HMM method
 - Paul Coen (pcoen)
   - Implementation for Part 1's simplified and HMM models. Initial Part 1 report.


## Part 1:
#### (1) A description of how you formulated the problem.
 - For the simplified model, we formulated this as trying to find max(P(S | W)) given the part-of-speech classes S and the training data containing the relations between S and W. This can be re-written using bayes law as P(S | W) = (P(W | S) * P(S)) / P(W) where S is the part-of-speech tag (class) and W is the word/token
 - For the HMM, we use the same calculation from above, but also consider the transition probabilities for each word given the max prior label we found and the current label being considered. We then calculated P(S|W) * P(transition | prior_word, current_word) * P(prior found probability). In the case of the first word in a sentence, we consider the initial transition probabilities, as seen from the first parts-of-speech in the training dataset. We can then maximize this for each label given the prior labels to find the most likely sequence to return. We then back-trace through the max values found to find this sequence.
#### (2) A brief description of how your program works.
 - For both models, we can gather the training data by looking at the per-word likelihood of given tags, the occurrence of a word in the dataset, and the transitions between words in the dataset. More detailed comments of this are given in the train function in pos_solver.py.
 - For the simplified model function, we calculate the most likely label for each word and append it to the sequence. Essentially maximizing P(S | W) given above. This will take at most the number of classes times the number of words to calculate.
 - For the HMM function, we calculate the most likely label given the prior labels by iterating through all 144 possible transitions for each part-of-speech for each word in the sequence. We append the most likely labels for each prior label at the current time in the sequence and then consider the next word. We re-calculate the priors for each word to ensure that other paths are still explored. We then go back through this list of values to back-trace the max sequence path. We build this in reverse and then reverse the order to return to the solver. An older version can consider the most likely path only (in a prior commit on here that does this), but it has slightly reduced accuracy. The runtime is again dependent on sequence length and number of sentences to evaluate.
 - For the posterior calculations, we can take the log probability of each word given a label and sum these together (based on the specific implementations of the simplified and HMM models). There are two special cases in that have to be considered too.
   - The first appears in both models and is when we get a word that is not in the training dataset. In this case, we add nothing to the transition probability (as the log of 0 is undefined) and in the case of HMM, we carry through it's prior probability as the current prior to prevent all probabilities following being zero.
   - The second case is when we are at the start of a sentence in the HMM. In this case, we must consider the initial transition probabilities mentioned before.
 - The runtime for all of this is a little under 15 seconds (for both training and testing all of the sentences on the large file) and with the older method (and a slightly reduced accuracy), we can run in under 5 seconds total.
#### (3) A discussion of any problems you faced, any assumptions, simplifications, and/or design decisions you made.
 - ~~We found it strange that the simplified model is generally out-performing the HMM (very slightly).~~
   - Originally, this was true, but when finding the max sequence, we were able to have the HMM outperform the simplified model in both metrics.
 - When trying to implement the viterbi algorithm, we found one piece of ambiguity that resulted in the two implementations for the HMM model.
   - The first considers the case that we only propagate from the max label(s) found (only paths/transitions from this one class node to the next). We can potentially have more than one if they have the same probability value, but it is likely a subset of the classes. This reduced computation time and appeared similar to one of the in-class examples. We determined this was the incorrect method.
   - The second considers the case where we propagate from all nodes given the maximal path that reached the node and find the per-class maximal path. This ensures that if there is an unusually high transition probability that wasn't from the max label, we take it into account in the transition paths. This method will check classes^2 number of values for each word, but this is only 144 cases for our 12 classes and does not add much to the runtime. This improved accuracy, but left us asking which is the correct method (both have similar linear runtime results). We discussed this with Professor Leake and determined this and the max sequence finding method was the correct approach.


## Part 2:
#### (1) A description of how you formulated the problem.
- Data Pre-processing:
  - First, we decided to use the bc.train text file from Part 1 for training of this model. That is because we were interested in learning patterns in the english language, so that file is a good representative of such language.
  - Note that this training is done on raw text, as opposed to images, since we are trying to learn language-specific patterns (e.g., most common character, etc.). Also, we disregard the part-of-speech tags before training on the text file by modifiying the read_data() function, as we are not interested in that anymore.
  - However, we notice that the Python split() method splits the sentences whenever it reads an empty space. Even though this splitting point could be changed, using the blank spaces is a good choice. To compensate, we make the following observation: a sentence of n words contains n - 1 spaces. Nonetheless, notice that punction is an exception, as you can see in the previous comma we just used. Even though the comma is considered its own word, there is no blank space between the word "exception" and the comma. Thus, every time we see a punctuation sign, we subtract 1 from the total number of spaces in order to return the correct count.
  - Moreover, we noticed that there exist a large number of white pixels around every character in the testing images, as we are using a padding of 14x25 provided to us. Thus, whenever we test similarity between an input picture and the ground-truth (which is the noise-free image of that character), all characters yielded a high similarity. That is, the similarity between different characters was high because of the number of white pixels they had in common due to the 14x25 bounding boxes we were using.
  - This was a problem because, as all characters had high similarity scores, the decision was being heavily influenced by the how common a character was. In other words, the letter 'e' and the blank space ' ' are the most common characters in the dataset, so the model was returning those two characters way too often. Therefore, when computing similarity scores, we decided to compare the pixels at coordinates (x, y) in each picture if and only if at least one of the pixels at locations (x, y) in the input image and ground-truth image was black.
  - The above observation improved performance as it made similarity scores more accurate, but the issue with the 'e' and ' ' persisted, as the number of times those are present in the training dataset is very large. Hence, whenever we compared two pixels at coordinates (x, y) in each picture, if the pixels are equal we increment the similarity counter by 1 (which is the same we were doing before), but if they are not equal we decrement it by 1. This penalized the difference between black and white pixels heavily, which made the similarity scores more relevant.
  - Logically, if a similarity score was less than 0 we just outputed 0 as the probability for that character (this was changed later due to the implementation of log probabilities, which wil be explained below).
  - Additionally, when processing the data we read every character to reconstruct the sentences instead of a tokenized approach to avoid issues with over-adding spaces and to remove any characters not in our class list.
#### (2) A brief description of how your program works.
 - Simplified model:
   - For this model we just implemented Bayes' Rule after pre-processing the data as above. Let c = character and p = pixels. Then for input p we want to output the most likely c. That is, we want to output max( P( c | p ) ) for all characters c. Using Bayes' Rule, we rewrite P( c | p ) as P( p | c ) * P(c) / P(p).
   - For P( p | c ), which is the probability of the pixel distribution given the character, we use the similarity scores as explained during the pre-processing. Note that we do this online for each input image fed to our model. Also recall that this similarity score is computed using the ground-truth of the pixel distribution for character c, which makes this compiutation correct.
   - Moreover, for P(c) we just used the number of times character c appears in the training data divided by the total number of characters (i.e., normalize it). This value was computed and stored in a dictionary during pre-processing.
   - Then, for P(p) we initially used 1/73, which is the probability of being fed this pixel distribution as there are 73 possible characters. Howevver, since this a constant factor, we dicarded it. Also, the reason why we discarded is because intuitively P(p) = 1 makes more sense, as the probability of feeding our model with p is 1 because we are indeed feeding it p. To make this more accurate, we could use the ground-truth of the pixel distribution for the character c and create thousands of copies of it with Gaussian noise. Theoretically, this will produce p after a certain number of samples. In such a case, P(p) would be the amount of times we saw a pixel distribution equal to p divided by the number of copies we created.
   - Lastly, instead of multplying decimal probabilities when using Bayes' Rule, we decided to use logarithms to make use of the speed and precision of their corresponding arithmetic operations. Hence, a log probability of -infinity was "equal" to a probability of 0, so in the case explained during pre-processing in which the similarity score was less than 0, we now output a very large negative number instead.
 - HMM Model
   - This is done almost identically to Part1, except here we utilize the bayes calculation given above for the HMM and utilize the transitions between characters (that are logged during training). Outside of Part 2 specific requirements, this HMM model operates identically to Part 1. To reitterate it, we calculate the most likely character given the prior characters by iterating through all 73^2 possible transitions for each character in self.truth to each character in the sequence. We append the most likely character for each prior character at the current time in the sequence and then consider the next character. We re-calculate the priors for each character to ensure that other paths are still explored. We then go back through this list of values to back-trace the max sequence path. We build this in reverse and then reverse the order of this character sequence to return to the solver. We only pass through a subset of possible characters from full sentence though. We utilize the same space-finding method as in the simplified model for Part 2 to determine spaces. This essentially passes through tokens to the HMM to determine the likely word or words they represent.
#### (3) A discussion of any problems you faced, any assumptions, simplifications, and/or design decisions you made.
 - Problems faced were mainly getting everything running during pre-processing, as there were less instrunctions on how to proceed about this problem.
 - Also, the similarity scores were an issue as described above.
 - Keeping track of the prior probabilties for all the character along with the max character itself was challenging. We added a transitioning dataset where we keep track of all the max prior probabilities and the characters and add that to the next transitioning step while trying to predict the next test character.
 - Moreover, since the amount of blank spaces ' ' is unreasonably large, we had to divide by a constant factor so that the model did not output ' ' so often. Additionally, if an input image for character c is over 55% white, we are outputting a ' ' automatically. We tested an ablation in which we did not perform such method and the results are similar, but this method is more accurate as it reduces the number of false positives. Due to these two extra pre-processing steps, some of our probabilities sum up to 1 - epsilon, where epsilon is a very small degree of error. We asked Dr. Leake whether this was acceptable, and explained to him that the reason we do this is because there is some noise even in the text file, so performing this pre-pricessing highly improves performance even though it introduces some small errors in the precision of some computations. We believe this is an acceptable trade-off for this this task.
